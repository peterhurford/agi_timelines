{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4a5d6b3-f5c7-4422-bcae-77134d4f4469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded libraries\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import squigglepy as sq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "from datetime import datetime, timedelta\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "O3_LAUNCH_DATE = datetime(2025, 4, 16)\n",
    "CLAUDE_3P7_LAUNCH_DATE = datetime(2025, 2, 24)\n",
    "\n",
    "\n",
    "def run_model(model, index_date=O3_LAUNCH_DATE):\n",
    "    samples = sq.sample(model, n=100_000, verbose=True)\n",
    "    pprint(sq.get_percentiles(samples, digits=0))\n",
    "    print('\\n-\\n')\n",
    "    samples_ = sq.get_percentiles(samples_to_date(samples, index_date=index_date))\n",
    "    samples_ = {k: v.strftime(\"%Y %b %d\") for k, v in samples_.items()}\n",
    "    pprint(samples_)\n",
    "    return samples\n",
    "\n",
    "\n",
    "def samples_to_date(samples, index_date=O3_LAUNCH_DATE):\n",
    "    date_converter = np.vectorize(lambda x: index_date + timedelta(days=int(np.ceil(x))))\n",
    "    return date_converter(samples)\n",
    "\n",
    "\n",
    "def calculate_doubling_time(start_task_length, agi_task_length, doubling_time, acceleration=1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    start_task_length : scalar or distribution\n",
    "        Current hours needed for the reference task.\n",
    "    agi_task_length : scalar or distribution\n",
    "        Hours required for the task at AGI.\n",
    "    initial_doubling_time : scalar or distribution (days)\n",
    "        Doubling time at the *current* capability level.\n",
    "    acceleration : scalar or distribution\n",
    "        Multiplicative factor applied to the doubling time *after every doubling*.\n",
    "        • 1.0  → constant exponential growth (baseline).\n",
    "        • <1.0 → doubling time shrinks, giving super‑exponential growth.\n",
    "        • >1.0 → growth slows over time.\n",
    "    \"\"\"\n",
    "    doublings_needed = sq.dist_log(agi_task_length / start_task_length) / np.log(2)\n",
    "    if acceleration == 1:\n",
    "        return doublings_needed * doubling_time\n",
    "    else:\n",
    "        return doubling_time * (1 - acceleration**doublings_needed) / (1 - acceleration)\n",
    "\n",
    "\n",
    "def calendar_days_for_doublings(\n",
    "    doublings: np.ndarray,\n",
    "    initial_doubling_time: float,\n",
    "    acceleration: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Super‑exponential growth law from Chin & You (2024).\"\"\"\n",
    "    if np.isclose(acceleration, 1.0):\n",
    "        # pure exponential (constant doubling time)\n",
    "        return doublings * initial_doubling_time\n",
    "    return (\n",
    "        initial_doubling_time\n",
    "        * (1 - acceleration ** doublings)\n",
    "        / (1 - acceleration)\n",
    "    )\n",
    "\n",
    "\n",
    "print('Loaded libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e0475ac-76ee-40d8-8dcb-af4fd2f311a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 100000/100000 [00:06<00:00, 14983.39it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 100000/100000 [00:07<00:00, 13054.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1095,\n",
      " 5: 1251,\n",
      " 10: 1341,\n",
      " 20: 1460,\n",
      " 30: 1552,\n",
      " 40: 1634,\n",
      " 50: 1717,\n",
      " 60: 1801,\n",
      " 70: 1896,\n",
      " 80: 2015,\n",
      " 90: 2191,\n",
      " 95: 2350,\n",
      " 99: 2677}\n",
      "\n",
      "-\n",
      "\n",
      "{1: '2028 Feb 25',\n",
      " 5: '2028 Jul 29',\n",
      " 10: '2028 Oct 27',\n",
      " 20: '2029 Feb 23',\n",
      " 30: '2029 May 26',\n",
      " 40: '2029 Aug 17',\n",
      " 50: '2029 Nov 07',\n",
      " 60: '2030 Jan 31',\n",
      " 70: '2030 May 06',\n",
      " 80: '2030 Sep 01',\n",
      " 90: '2031 Feb 24',\n",
      " 95: '2031 Aug 02',\n",
      " 99: '2032 Jun 25'}\n"
     ]
    }
   ],
   "source": [
    "def metr_model():\n",
    "    days = calculate_doubling_time(start_task_length=1, agi_task_length=167, doubling_time=212, acceleration=1) # Variables from METR paper\n",
    "    measurement_error_variance = sq.invlognorm(0.8, 1.5) # Add easurement error on tasks: SD fit to trend variance from Figure 12\n",
    "    return days * measurement_error_variance\n",
    "\n",
    "_ = run_model(metr_model, index_date=CLAUDE_3P7_LAUNCH_DATE) # Results should look similar to Figure 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97cca18c-09a0-4cda-aee2-f1fedb464204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 100000/100000 [00:06<00:00, 14891.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 100000/100000 [00:07<00:00, 12856.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 545,\n",
      " 5: 621,\n",
      " 10: 666,\n",
      " 20: 724,\n",
      " 30: 768,\n",
      " 40: 809,\n",
      " 50: 849,\n",
      " 60: 891,\n",
      " 70: 939,\n",
      " 80: 998,\n",
      " 90: 1085,\n",
      " 95: 1164,\n",
      " 99: 1321}\n",
      "\n",
      "-\n",
      "\n",
      "{1: '2026 Oct 14',\n",
      " 5: '2026 Dec 28',\n",
      " 10: '2027 Feb 11',\n",
      " 20: '2027 Apr 10',\n",
      " 30: '2027 May 25',\n",
      " 40: '2027 Jul 05',\n",
      " 50: '2027 Aug 14',\n",
      " 60: '2027 Sep 25',\n",
      " 70: '2027 Nov 11',\n",
      " 80: '2028 Jan 09',\n",
      " 90: '2028 Apr 06',\n",
      " 95: '2028 Jun 24',\n",
      " 99: '2028 Nov 28'}\n"
     ]
    }
   ],
   "source": [
    "def metr_model_with_o3():\n",
    "    days = calculate_doubling_time(start_task_length=1.75, agi_task_length=167, doubling_time=118, acceleration=1) # Use o3 task length, o3 launch date, and the 2024-2025 doubling time\n",
    "    measurement_error_variance = sq.invlognorm(0.8, 1.5) # Add measurement error on tasks: SD fit to trend variance from Figure 12\n",
    "    return days * measurement_error_variance\n",
    "\n",
    "_ = run_model(metr_model_with_o3, index_date=O3_LAUNCH_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9258965e-db47-4631-bf6c-63e76b9221bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 100000/100000 [00:01<00:00, 89965.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 100000/100000 [00:04<00:00, 20321.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1778,\n",
      " 5: 1778,\n",
      " 10: 1778,\n",
      " 20: 1778,\n",
      " 30: 1778,\n",
      " 40: 1778,\n",
      " 50: 1778,\n",
      " 60: 1778,\n",
      " 70: 1778,\n",
      " 80: 1778,\n",
      " 90: 1778,\n",
      " 95: 1778,\n",
      " 99: 1778}\n",
      "\n",
      "-\n",
      "\n",
      "{1: '2030 Feb 28',\n",
      " 5: '2030 Feb 28',\n",
      " 10: '2030 Feb 28',\n",
      " 20: '2030 Feb 28',\n",
      " 30: '2030 Feb 28',\n",
      " 40: '2030 Feb 28',\n",
      " 50: '2030 Feb 28',\n",
      " 60: '2030 Feb 28',\n",
      " 70: '2030 Feb 28',\n",
      " 80: '2030 Feb 28',\n",
      " 90: '2030 Feb 28',\n",
      " 95: '2030 Feb 28',\n",
      " 99: '2030 Feb 28'}\n"
     ]
    }
   ],
   "source": [
    "# The simpler model with static variables from my Substack\n",
    "def simple_model():\n",
    "    days = calculate_doubling_time(start_task_length=3.75/60, agi_task_length=167, doubling_time=165, acceleration=1)\n",
    "    shift = 100\n",
    "    return days - shift\n",
    "\n",
    "_ = run_model(simple_model, index_date=O3_LAUNCH_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd9db226-d328-45c8-a35a-0a416c04b6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## START task length (displayed in min) ##\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.02,\n",
       " 5: 0.02,\n",
       " 10: 0.02,\n",
       " 20: 0.06,\n",
       " 30: 0.14,\n",
       " 40: 0.29,\n",
       " 50: 0.56,\n",
       " 60: 1.08,\n",
       " 70: 2.23,\n",
       " 80: 4.32,\n",
       " 90: 12.11,\n",
       " 95: 31.5,\n",
       " 99: 126.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('## START task length (displayed in min) ##')\n",
    "\n",
    "# -- DEFINE CURRENT BEST\n",
    "current_best = 1.75 # o3 task length at 50% reliability?\n",
    "\n",
    "# -- DEFINE ADJUSTMENTS\n",
    "elicitation_boost = sq.mixture([[0.3, 1], # Can you get a boost to scores by iterating on scaffolding and other elicitation techniques? 30% chance no, 40% chance you can get a 1.2x speed up, 30% chance of 1.5x.\n",
    "                                [0.4, 1.2],\n",
    "                                [0.3, 1.5]])\n",
    "                               \n",
    "reliability_needed = sq.mixture([[0.2, 0.5], # What amount of reliability will we need? Probability distribution over hypotheses\n",
    "                                 [0.4, 0.8],\n",
    "                                 [0.2, 0.9],\n",
    "                                 [0.1, 0.95],\n",
    "                                 [0.1, 0.99]])\n",
    "\n",
    "def reliability_count_to_penalty(reliability):\n",
    "    r = np.asarray(reliability, dtype=float)\n",
    "    reliability = np.array([0.50, 0.80, 0.90, 0.95, 0.99])\n",
    "    penalty = np.array([1.0, 0.25, 0.25**2, 0.25**3, 0.25**4])\n",
    "    matches = r[..., None] == reliability\n",
    "    hit_any = matches.any(axis=-1)\n",
    "    idx = matches.argmax(axis=-1)\n",
    "    out = np.full_like(r, np.nan, dtype=float)\n",
    "    out[hit_any] = penalty[idx[hit_any]]\n",
    "    return out\n",
    "\n",
    "task_type_penalty = sq.mixture([[0.1, 1],                         # 10% chance that METR's software tasks are sufficient for AGI\n",
    "                                [0.3, 1 / sq.lognorm(5, 20)],     # 30% chance that true AGI tasks are 5-20x harder than METR's software tasks\n",
    "                                [0.6, 1 / sq.lognorm(10, 1000)]]) # 60% chance that true AGI tasks are 10-1000x harder than METR's software tasks\n",
    "\n",
    "# -- CREATE DISTRIBUTION\n",
    "# Start with current best, add elicitation boost\n",
    "start_task_length = current_best * elicitation_boost\n",
    "\n",
    "# add reliability penalty\n",
    "start_task_length = start_task_length * sq.dist_fn(reliability_needed, reliability_count_to_penalty)\n",
    "\n",
    "# Add task type penalty\n",
    "start_task_length *= task_type_penalty\n",
    "\n",
    "# Add a minimum value of 1sec\n",
    "start_task_length = sq.dist_max(1/60/60, start_task_length)\n",
    "\n",
    "# Show samples in minutes (naturally in hours)\n",
    "sq.get_percentiles((start_task_length * 60) @ 100_000, digits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b91a8e4-007b-4b91-aba9-237102467302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## AGI task length (displayed in hrs) ##\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 40,\n",
       " 5: 50,\n",
       " 10: 79,\n",
       " 20: 138,\n",
       " 30: 206,\n",
       " 40: 291,\n",
       " 50: 399,\n",
       " 60: 550,\n",
       " 70: 770,\n",
       " 80: 1142,\n",
       " 90: 1973,\n",
       " 95: 3105,\n",
       " 99: 7325}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('## AGI task length (displayed in hrs) ##')\n",
    "agi_task_length = sq.lognorm(80, 2000, credibility=80, lclip=40)\n",
    "sq.get_percentiles(agi_task_length @ 100_000, digits=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aacf9251-36ef-4b49-a07c-9ef269defe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## DOUBLING TIME (displayed in days) ##\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 118,\n",
       " 5: 118,\n",
       " 10: 118,\n",
       " 20: 157,\n",
       " 30: 181,\n",
       " 40: 209,\n",
       " 50: 212,\n",
       " 60: 212,\n",
       " 70: 212,\n",
       " 80: 212,\n",
       " 90: 320,\n",
       " 95: 320,\n",
       " 99: 320}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('## DOUBLING TIME (displayed in days) ##')\n",
    "doubling_time = sq.mixture([[0.4, 212],\n",
    "                            [0.1, 118],\n",
    "                            [0.1, 320],\n",
    "                            [0.4, sq.lognorm(lognorm_mean=185.25, lognorm_sd=40)]])\n",
    "sq.get_percentiles(doubling_time @ 100_000, digits=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "151bb954-9b59-4dc5-bc20-fec1a8965124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ACCELERATION (displayed in days)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0.896,\n",
       " 5: 0.978,\n",
       " 10: 1.0,\n",
       " 20: 1.0,\n",
       " 30: 1.0,\n",
       " 40: 1.0,\n",
       " 50: 1.0,\n",
       " 60: 1.0,\n",
       " 70: 1.0,\n",
       " 80: 1.0,\n",
       " 90: 1.0,\n",
       " 95: 1.022,\n",
       " 99: 1.099}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('## ACCELERATION (displayed in days)')\n",
    "acceleration = sq.mixture([[0.1, 1 + sq.lognorm(0.005, 0.1, credibility=80)],\n",
    "                           [0.8, 1],\n",
    "                           [0.1, 1 - sq.lognorm(0.005, 0.1, credibility=80)]])\n",
    "sq.get_percentiles(acceleration @ 100_000, digits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48d0ea11-e4a4-4f51-bc01-0c1e1c7d68e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## SHIFT (displayed in days) ##\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 5: 14,\n",
       " 10: 30,\n",
       " 20: 51,\n",
       " 30: 66,\n",
       " 40: 78,\n",
       " 50: 90,\n",
       " 60: 102,\n",
       " 70: 115,\n",
       " 80: 130,\n",
       " 90: 150,\n",
       " 95: 167,\n",
       " 99: 199}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('## SHIFT (displayed in days) ##')\n",
    "shift = sq.norm(30, 30*5, credibility=80, lclip=0)\n",
    "sq.get_percentiles(shift @ 100_000, digits=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9a416d-b871-411e-a806-13caaf19790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 100000/100000 [00:08<00:00, 12204.79it/s]\n",
      " 25%|██████████████████▊                                                         | 24704/100000 [00:18<00:55, 1359.00it/s]"
     ]
    }
   ],
   "source": [
    "def adapted_metr_model():\n",
    "    start_task_length_ = start_task_length * (2 ** (shift / doubling_time))\n",
    "    days = calculate_doubling_time(start_task_length_, agi_task_length, doubling_time, acceleration)\n",
    "    measurement_error_variance = sq.invlognorm(0.8, 1.5) # Add measurement error on tasks: SD fit to trend variance from Figure 12\n",
    "    return days * measurement_error_variance\n",
    "\n",
    "samples = run_model(adapted_metr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c0bf0-9e89-4568-95f4-098bf15b8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('## DISTRIBUTION OF AGI ARRIVAL DATE ##')\n",
    "# Convert datetime samples to years\n",
    "agi_years = [s / 365 + 2025 for s in samples]\n",
    "pctiles = sq.get_percentiles(agi_years, percentiles=[1, 2, 3, 4, 5, 10, 15, 20, 25, 35, 50, 60, 75, 80, 90, 95])\n",
    "pprint([\n",
    "    ((str(o[0]) + '%: ' + str(round(o[1], 1))) if o[1] < 2100 else '>2100') \n",
    "    for o in pctiles.items()\n",
    "])\n",
    "print('')\n",
    "print('')\n",
    "print('## DISTRIBUTION OF RELATIVE AGI ARRIVAL DATE ##')\n",
    "pprint([\n",
    "    ((str(o[0]) + '%: ' + str(round(o[1] - 2025, 1))) if o[1] < 2100 else '>75') + ' years from now' \n",
    "    for o in pctiles.items()\n",
    "])\n",
    "print('(Mean: {} years from now)'.format(int(round(np.mean([t - 2025 for t in agi_years])))))\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "\n",
    "print('## AGI ARRIVAL DATE BY BIN ##')\n",
    "\n",
    "def bin_agi_yrs(low=None, hi=None):\n",
    "    low = 2025 if low is None else low\n",
    "    if hi is None:\n",
    "        r = np.mean([y >= low for y in agi_years])\n",
    "    else:\n",
    "        r = np.mean([(y >= low) and (y <= hi) for y in agi_years])\n",
    "    return round(r * 100, 1)\n",
    "\n",
    "\n",
    "year_pairs = [[2025, 2026],\n",
    "              [2026, 2027],\n",
    "              [2027, 2028],\n",
    "              [2028, 2029],\n",
    "              [2029, 2030],\n",
    "              [2030, 2032],\n",
    "              [2032, 2035],\n",
    "              [2035, 2040],\n",
    "              [2040, 2050],\n",
    "              [2050, 2060],\n",
    "              [2060, 2070],\n",
    "              [2070, 2080],\n",
    "              [2080, 2090],\n",
    "              [2090, 2100]]\n",
    "for y in year_pairs:\n",
    "    if y[0] == y[1] - 1:\n",
    "        print('{}: {}%'.format(y[0], bin_agi_yrs(y[0], y[1])))\n",
    "    else:\n",
    "        print('{}-{}: {}%'.format(y[0], y[1]-1, bin_agi_yrs(y[0], y[1])))\n",
    "print('>{}: {}%'.format(2100, bin_agi_yrs(low=2100)))\n",
    "print('')\n",
    "print('')\n",
    "\n",
    "print('## AGI ARRIVAL DATE BY YEAR ##')\n",
    "for y in list(range(2025, 2035)) + list(range(2035, 2100, 5)):\n",
    "    print('By EOY {}: {}%'.format(y, bin_agi_yrs(hi=y+1)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c77d95a-ca35-4503-889e-1f920e029ecb",
   "metadata": {},
   "source": [
    "## Overall prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d01b0-192d-4a6b-893c-4ac4a5ec5961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import squigglepy as sq\n",
    "\n",
    "DAYS_PER_QUARTER = 365 / 4\n",
    "\n",
    "def billions_formatter(x, pos):\n",
    "    if x >= 1e9:\n",
    "        return f\"{x/1e9:.1f}B\"\n",
    "    if x >= 1e6:\n",
    "        return f\"{x/1e6:.1f}M\"\n",
    "    if x >= 1e3:\n",
    "        return f\"{x/1e3:.1f}K\"\n",
    "    if x <= 0.5:\n",
    "        return f\"1/{1/x:.0f}\"\n",
    "    return f\"{x:.0f}\"\n",
    "\n",
    "\n",
    "def _quarter_labels(n: int, start_year: int = 2025) -> list[str]:\n",
    "    return [f\"{start_year + q // 4}Q{q % 4 + 1}\" for q in range(n + 1)]\n",
    "\n",
    "\n",
    "def _y_ticks(lo: int, hi: int) -> list[int]:\n",
    "    return [2 ** k for k in range(lo, hi + 1)]\n",
    "\n",
    "\n",
    "def _first_curve(order, traj, reference, above):\n",
    "    cmp = np.greater_equal if above else np.less_equal\n",
    "    for idx in order:\n",
    "        if np.all(cmp(traj[idx], reference)):\n",
    "            return idx\n",
    "    return order[0]\n",
    "\n",
    "\n",
    "def plot_exponential_growth(\n",
    "    doubling_time_days,\n",
    "    starting_hours,\n",
    "    agi_task_length,\n",
    "    shift=0,\n",
    "    acceleration=1,\n",
    "    n_quarters: int = 40,\n",
    "    n_samples: int = 10_000,\n",
    "    n_traces: int = 100,\n",
    "    max_task_power: int = 13,\n",
    "    min_y_power: int = -8,\n",
    ") -> None:\n",
    "    max_task_hours = 2 ** max_task_power\n",
    "    tau0 = sq.sample(doubling_time_days, n=n_samples)\n",
    "    accel = sq.sample(acceleration, n=n_samples)\n",
    "    shift = sq.sample(shift, n=n_samples)\n",
    "    agi = sq.sample(sq.dist_min(max_task_hours, agi_task_length), n=n_samples)\n",
    "    start = sq.sample(starting_hours, n=n_samples) * 2 ** (shift / tau0)\n",
    "\n",
    "    quarters = np.arange(n_quarters + 1)\n",
    "    traj = np.zeros((n_samples, len(quarters)))\n",
    "    clip_idx = np.full(n_samples, len(quarters), dtype=int)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        tau = tau0[i]\n",
    "        val = start[i]\n",
    "        for j in range(len(quarters)):\n",
    "            if val >= max_task_hours:\n",
    "                traj[i, j:] = max_task_hours\n",
    "                clip_idx[i] = j\n",
    "                break\n",
    "            traj[i, j] = val\n",
    "            val *= 2 ** (DAYS_PER_QUARTER / tau)\n",
    "            tau *= accel[i]\n",
    "\n",
    "    reached = traj >= agi[:, None]\n",
    "    first_hit = np.argmax(reached, axis=1)\n",
    "    first_hit[np.all(~reached, axis=1)] = len(quarters)\n",
    "\n",
    "    order = np.argsort(first_hit)\n",
    "    median_idx = order[len(order) // 2]\n",
    "    median_curve = traj[median_idx]\n",
    "\n",
    "    idx10 = _first_curve(order[int(0.10 * n_samples):], traj, median_curve, above=True)\n",
    "    idx90 = _first_curve(order[: int(0.90 * n_samples)][::-1], traj, median_curve, above=False)\n",
    "\n",
    "    highlights = {\n",
    "        \"10 % earliest\": (traj[idx10], first_hit[idx10], clip_idx[idx10], \"b--\"),\n",
    "        \"Median\": (median_curve, first_hit[median_idx], clip_idx[median_idx], \"b-\"),\n",
    "        \"90 % latest\": (traj[idx90], first_hit[idx90], clip_idx[idx90], \"b--\"),\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(11, 6))\n",
    "    rng = np.random.default_rng()\n",
    "\n",
    "    for i in rng.choice(n_samples, min(n_traces, n_samples), replace=False):\n",
    "        end_q = min(first_hit[i], clip_idx[i], len(quarters) - 1)\n",
    "        plt.plot(\n",
    "            quarters[: end_q + 1],\n",
    "            traj[i, : end_q + 1],\n",
    "            color=\"tab:blue\",\n",
    "            lw=0.3,\n",
    "            alpha=0.25,\n",
    "        )\n",
    "        marker = \"rx\" if first_hit[i] < len(quarters) else \"ko\"\n",
    "        plt.plot(quarters[end_q], traj[i, end_q], marker, ms=4, alpha=0.6)\n",
    "\n",
    "    for label, (curve, hit_q, clip_q, style) in highlights.items():\n",
    "        end_q = min(hit_q, clip_q, len(quarters) - 1)\n",
    "        plt.plot(quarters[: end_q + 1], curve[: end_q + 1], style, lw=2, label=label)\n",
    "        marker = \"rx\" if hit_q < len(quarters) else \"ko\"\n",
    "        plt.plot(quarters[end_q], curve[end_q], marker, ms=7)\n",
    "\n",
    "    plt.plot([], [], \"rx\", ms=7, label=\"AGI reached\")\n",
    "    plt.plot([], [], \"ko\", ms=7, label=f\"AGI not by {_quarter_labels(n_quarters)[-1]}\")\n",
    "    plt.yscale(\"log\", base=2)\n",
    "    \n",
    "    nonzero_values = traj[traj > 0]\n",
    "    percentile_0p1 = np.percentile(nonzero_values, 0.1)\n",
    "    min_y_data = int(np.floor(np.log2(percentile_0p1)))\n",
    "    y_lo = max(min_y_power, min_y_data)\n",
    "    \n",
    "    plt.yticks(_y_ticks(lo=y_lo, hi=max_task_power))\n",
    "    plt.gca().yaxis.set_major_formatter(FuncFormatter(billions_formatter))\n",
    "    plt.xticks(quarters, _quarter_labels(n_quarters), rotation=90)\n",
    "    plt.grid(ls=\"--\", alpha=0.7)\n",
    "    plt.ylabel(\"Task length (hours) -- note log scale\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "plot_exponential_growth(\n",
    "    doubling_time_days=doubling_time,\n",
    "    starting_hours=start_task_length,\n",
    "    agi_task_length=agi_task_length,\n",
    "    shift=shift,\n",
    "    acceleration=acceleration,\n",
    "    n_quarters=51,\n",
    "    n_samples=100_000,\n",
    "    n_traces=200,\n",
    "    max_task_power=13,\n",
    "    min_y_power=-15,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784c8790-a042-4c7a-9dcd-1299a6c1b176",
   "metadata": {},
   "source": [
    "## Project next METR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50391457-7b99-46c4-bb4c-c80d80a690e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate quarterly METR predictions (PUBLIC MODELS - NO SHIFT)\n",
    "print(\"METR Task Horizon Predictions (50% reliability) - PUBLIC MODELS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Baseline: o3 at 1.75hr on {O3_LAUNCH_DATE.strftime('%Y-%m-%d')}\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Define quarters to predict (END of quarter)\n",
    "def end_of_quarter(year, q):\n",
    "    if q == 1:\n",
    "        return datetime(year, 3, 31)\n",
    "    elif q == 2:\n",
    "        return datetime(year, 6, 30)\n",
    "    elif q == 3:\n",
    "        return datetime(year, 9, 30)\n",
    "    else:  # q == 4\n",
    "        return datetime(year, 12, 31)\n",
    "\n",
    "quarters = []\n",
    "# Start with Q2 2025 since o3 launched in April\n",
    "for year in range(2025, 2030):\n",
    "    start_q = 2 if year == 2025 else 1\n",
    "    for q in range(start_q, 5):\n",
    "        quarters.append((year, q, end_of_quarter(year, q)))\n",
    "\n",
    "# Sample parameters\n",
    "n_samples = 50000\n",
    "tau_samples = sq.sample(doubling_time, n=n_samples)\n",
    "accel_samples = sq.sample(acceleration, n=n_samples)\n",
    "\n",
    "print(f\"{'Quarter':<8} {'End Date':<12} {'Mean':<10} {'Median':<10} {'90% CI':<25} {'Days'}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "results = []\n",
    "for year, q, quarter_date in quarters:\n",
    "    days_from_o3 = (quarter_date - O3_LAUNCH_DATE).days\n",
    "    \n",
    "    if days_from_o3 < 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate task length for each sample\n",
    "    # NO SHIFT for public models\n",
    "    effective_days = days_from_o3  # Changed: removed + shift_samples\n",
    "    \n",
    "    # Use the calculate_doubling_time function logic\n",
    "    task_lengths = np.zeros(n_samples)\n",
    "    for i in range(n_samples):\n",
    "        if accel_samples[i] == 1:\n",
    "            # Simple exponential\n",
    "            doublings = effective_days / tau_samples[i]\n",
    "            task_lengths[i] = 1.75 * (2 ** doublings)\n",
    "        else:\n",
    "            # Superexponential - use binary search\n",
    "            low, high = 0, 100\n",
    "            target_days = effective_days\n",
    "            tau0 = tau_samples[i]\n",
    "            a = accel_samples[i]\n",
    "            \n",
    "            for _ in range(50):\n",
    "                mid = (low + high) / 2\n",
    "                if a == 1:\n",
    "                    predicted_days = mid * tau0\n",
    "                else:\n",
    "                    predicted_days = tau0 * (1 - a**mid) / (1 - a)\n",
    "                \n",
    "                if predicted_days < target_days:\n",
    "                    low = mid\n",
    "                else:\n",
    "                    high = mid\n",
    "            \n",
    "            doublings = (low + high) / 2\n",
    "            task_lengths[i] = 1.75 * (2 ** doublings)\n",
    "    \n",
    "    # Cap at reasonable maximum\n",
    "    task_lengths = np.minimum(task_lengths, 2000)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_val = np.mean(task_lengths)\n",
    "    median_val = np.median(task_lengths)\n",
    "    p5 = np.percentile(task_lengths, 5)\n",
    "    p95 = np.percentile(task_lengths, 95)\n",
    "    \n",
    "    def fmt_time(hrs):\n",
    "        if hrs < 1:\n",
    "            return f\"{int(hrs * 60)}min\"\n",
    "        elif hrs < 24:\n",
    "            return f\"{hrs:.1f}hr\"\n",
    "        elif hrs < 168:\n",
    "            return f\"{hrs/24:.1f}d\"\n",
    "        else:\n",
    "            return f\"{hrs/168:.1f}wk\"\n",
    "    \n",
    "    ci_str = f\"[{fmt_time(p5)} - {fmt_time(p95)}]\"\n",
    "    date_str = quarter_date.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    print(f\"{year}Q{q:<7} {date_str:<12} {fmt_time(mean_val):<10} {fmt_time(median_val):<10} {ci_str:<25} {days_from_o3:>4}\")\n",
    "    \n",
    "    results.append({\n",
    "        'quarter': f\"{year}Q{q}\",\n",
    "        'date': quarter_date,\n",
    "        'mean': mean_val,\n",
    "        'median': median_val,\n",
    "        'p5': p5,\n",
    "        'p95': p95,\n",
    "        'days': days_from_o3,\n",
    "        'task_lengths': task_lengths\n",
    "    })\n",
    "    \n",
    "    if q == 4:\n",
    "        print()\n",
    "\n",
    "# Show key milestones\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Key Milestones (Median Estimates) - PUBLIC MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "milestones = [(2, \"2 hours\"), (4, \"4 hours\"), (8, \"8 hours\"), \n",
    "              (24, \"1 day\"), (168, \"1 week\"), (730, \"1 month\")]\n",
    "\n",
    "for hours, name in milestones:\n",
    "    for r in results:\n",
    "        if r['median'] >= hours:\n",
    "            prob = np.mean(r['task_lengths'] >= hours) * 100\n",
    "            date_str = r['date'].strftime(\"%B %d, %Y\")\n",
    "            print(f\"{name:<12} → {r['quarter']} ({date_str}, {prob:.0f}% of samples)\")\n",
    "            break\n",
    "    else:\n",
    "        print(f\"{name:<12} → Not reached by 2029Q4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5810f144-1566-4945-92f9-dee75fc8b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_exponential_growth(\n",
    "    doubling_time_days=doubling_time,\n",
    "    starting_hours=1.75,\n",
    "    agi_task_length=100_000,\n",
    "    shift=0,\n",
    "    acceleration=acceleration,\n",
    "    n_quarters=16,\n",
    "    n_samples=100_000,\n",
    "    n_traces=200,\n",
    "    max_task_power=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffac563b-ee18-4a5b-a828-d19f24b16c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check WITH SHIFT=0 for public models\n",
    "today = datetime(2025, 7, 1)\n",
    "days_since_o3 = (today - O3_LAUNCH_DATE).days\n",
    "\n",
    "print(f\"Days since o3: {days_since_o3}\")\n",
    "print(f\"Model predictions for PUBLIC models on July 1, 2025:\\n\")\n",
    "\n",
    "# Calculate for today WITH NO SHIFT\n",
    "task_lengths_today_public = np.zeros(n_samples)\n",
    "\n",
    "for i in range(n_samples):\n",
    "    if accel_samples[i] == 1:\n",
    "        doublings = days_since_o3 / tau_samples[i]\n",
    "        task_lengths_today_public[i] = 1.75 * (2 ** doublings)\n",
    "    else:\n",
    "        # Binary search for doublings\n",
    "        low, high = 0, 100\n",
    "        target_days = days_since_o3  # NO SHIFT\n",
    "        tau0 = tau_samples[i]\n",
    "        a = accel_samples[i]\n",
    "        \n",
    "        for _ in range(50):\n",
    "            mid = (low + high) / 2\n",
    "            predicted_days = tau0 * (1 - a**mid) / (1 - a) if a != 1 else mid * tau0\n",
    "            \n",
    "            if predicted_days < target_days:\n",
    "                low = mid\n",
    "            else:\n",
    "                high = mid\n",
    "        \n",
    "        doublings = (low + high) / 2\n",
    "        task_lengths_today_public[i] = 1.75 * (2 ** doublings)\n",
    "\n",
    "task_lengths_today_public = np.minimum(task_lengths_today_public, 2000)\n",
    "\n",
    "print(f\"5th percentile:  {np.percentile(task_lengths_today_public, 5):.2f} hours\")\n",
    "print(f\"Median:          {np.median(task_lengths_today_public):.2f} hours\")\n",
    "print(f\"Mean:            {np.mean(task_lengths_today_public):.2f} hours\")\n",
    "print(f\"95th percentile: {np.percentile(task_lengths_today_public, 95):.2f} hours\")\n",
    "\n",
    "print(f\"\\nActual observed: Claude 4 at ~1.1-1.3 hours\")\n",
    "print(f\"\\nGap: {np.median(task_lengths_today_public):.2f}hr predicted vs ~1.2hr actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9407c3-510b-4792-933d-ee14581abf51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
