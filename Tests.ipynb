{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "341eaab9-1348-40b6-9803-34851bd5e0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded libraries\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Tuple\n",
    "from scipy.optimize import minimize, Bounds\n",
    "\n",
    "\n",
    "def _pretty_time(hours: float) -> str:\n",
    "    \"\"\"Return a string with value + unit, choosing h / min / s.\"\"\"\n",
    "    if hours >= 1:\n",
    "        return f\"{hours:6.2f}hr\"\n",
    "    minutes = hours * 60\n",
    "    if minutes >= 1:\n",
    "        return f\"{minutes:6.2f}min\"\n",
    "    seconds = minutes * 60\n",
    "    return f\"{seconds:6.0f}sec\"\n",
    "\n",
    "    \n",
    "def test_acceleration(\n",
    "    start_task_length: float,\n",
    "    agi_task_length: float,\n",
    "    initial_doubling_time: float,\n",
    "    acceleration: float = 1.0,\n",
    "    start_date: str | datetime | None = None,\n",
    "    date_fmt: str = \"%Y‑%m‑%d\",\n",
    "):\n",
    "    # Anchor date\n",
    "    if start_date is None:\n",
    "        start_date = datetime.today()\n",
    "    elif isinstance(start_date, str):\n",
    "        start_date = datetime.fromisoformat(start_date)\n",
    "\n",
    "    current_task = start_task_length\n",
    "    days_elapsed = 0.0\n",
    "    tau = initial_doubling_time\n",
    "    step = 0\n",
    "\n",
    "    header = f\"{'Step':>4} | {'Date':^10} | {'Day':>6} | {'Task':>10} | τ (d)\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    while current_task < agi_task_length:\n",
    "        date = start_date + timedelta(days=days_elapsed)\n",
    "        print(f\"{step:4d} | {date.strftime(date_fmt)} | \"\n",
    "              f\"{int(days_elapsed):6d} | {_pretty_time(current_task):>10} | {tau:5.1f}\")\n",
    "\n",
    "        current_task *= 2            # actual doubling\n",
    "        days_elapsed += tau\n",
    "        tau *= acceleration          # super‑/sub‑exponential effect\n",
    "        step += 1\n",
    "\n",
    "    # final line after exceeding target\n",
    "    date = start_date + timedelta(days=days_elapsed)\n",
    "    print(f\"{step:4d} | {date.strftime(date_fmt)} | \"\n",
    "          f\"{int(days_elapsed):6d} | {_pretty_time(current_task):>10} | {tau:5.1f}  <-- reached target\")\n",
    "\n",
    "\n",
    "def estimate_growth_parameters(\n",
    "    observations,\n",
    "    baseline_date=None,\n",
    "    baseline_task_hours=None,\n",
    "    reliability_level=\"50%\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns (initial_doubling_time, acceleration) where:\n",
    "    - initial_doubling_time = doubling time in days at baseline capability\n",
    "    - acceleration = multiplicative change after each doubling\n",
    "    \"\"\"\n",
    "    # Pick the right column based on reliability level\n",
    "    if reliability_level == \"50%\":\n",
    "        hours_idx = 2\n",
    "    elif reliability_level == \"80%\":\n",
    "        hours_idx = 3\n",
    "    else:\n",
    "        raise ValueError(\"reliability_level must be '50%' or '80%'\")\n",
    "    \n",
    "    clean_data = [(name, date, obs[hours_idx]) for obs in observations for name, date in [(obs[0], obs[1])]]\n",
    "    \n",
    "    if baseline_date is None:\n",
    "        baseline_date = clean_data[0][1]\n",
    "    if baseline_task_hours is None:\n",
    "        baseline_task_hours = clean_data[0][2]\n",
    "\n",
    "    doublings = np.log([hours / baseline_task_hours for _, _, hours in clean_data]) / np.log(2)\n",
    "    elapsed_days = np.array([(date - baseline_date).days for _, date, _ in clean_data], dtype=float)\n",
    "\n",
    "    def mse_loss(params):\n",
    "        doubling_time, accel = params\n",
    "        if doubling_time <= 0 or not 0 < accel < 2:\n",
    "            return np.inf\n",
    "        if np.isclose(accel, 1.0):\n",
    "            prediction = doublings * doubling_time\n",
    "        else:\n",
    "            prediction = doubling_time * (1 - accel**doublings) / (1 - accel)\n",
    "        return np.mean((prediction - elapsed_days)**2)\n",
    "\n",
    "    bounds = Bounds([1e-6, 0.9], [np.inf, 1.0])\n",
    "    result = minimize(mse_loss, x0=[260.0, 0.95], method=\"L-BFGS-B\", bounds=bounds)    \n",
    "    doubling_time, acceleration = result.x\n",
    "    return round(doubling_time), round(acceleration, 3)\n",
    "\n",
    "\n",
    "def print_estimation(data, reliability_level=\"50%\"):\n",
    "    start = data[0][0]\n",
    "    end = data[-1][0]\n",
    "    params = estimate_growth_parameters(data, reliability_level=reliability_level)\n",
    "    print(f\"{start} to {end} ({reliability_level}): {params}\")\n",
    "\n",
    "\n",
    "def bootstrap_growth_parameters(observations, n_bootstrap=1000, reliability_level=\"50%\", \n",
    "                               min_models=5, recent_weight=2.0):\n",
    "    \"\"\"\n",
    "    Bootstrap confidence intervals for growth parameters with time-based weighting.\n",
    "    \"\"\"\n",
    "    n_obs = len(observations)\n",
    "    results = []\n",
    "    \n",
    "    # Weight more recent observations higher\n",
    "    weights = np.array([recent_weight ** (i / n_obs) for i in range(n_obs)])\n",
    "    weights /= weights.sum()\n",
    "    \n",
    "    for _ in range(n_bootstrap):\n",
    "        # Sample with replacement, weighted by recency\n",
    "        indices = np.random.choice(n_obs, size=n_obs, replace=True, p=weights)\n",
    "        bootstrap_sample = [observations[i] for i in sorted(indices)]\n",
    "        \n",
    "        # Only fit if we have enough unique models and reasonable time span\n",
    "        if len(set(indices)) >= min_models:\n",
    "            try:\n",
    "                params = estimate_growth_parameters(bootstrap_sample, reliability_level=reliability_level)\n",
    "                if params[0] < 1000:  # Filter out degenerate fits\n",
    "                    results.append(params)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    if not results:\n",
    "        return None\n",
    "    \n",
    "    # Calculate percentiles\n",
    "    results = np.array(results)\n",
    "    percentiles = np.percentile(results, [2.5, 50, 97.5], axis=0)\n",
    "    \n",
    "    return {\n",
    "        'median': (round(percentiles[1, 0]), round(percentiles[1, 1], 3)),\n",
    "        'ci_95': {\n",
    "            'doubling_time': (round(percentiles[0, 0]), round(percentiles[2, 0])),\n",
    "            'acceleration': (round(percentiles[0, 1], 3), round(percentiles[2, 1], 3))\n",
    "        },\n",
    "        'mean': (round(results[:, 0].mean()), round(results[:, 1].mean(), 3)),\n",
    "        'std': (round(results[:, 0].std()), round(results[:, 1].std(), 3))\n",
    "    }\n",
    "\n",
    "\n",
    "def sliding_window_analysis(observations, window_sizes=[6, 8, 10, 12], reliability_level=\"50%\"):\n",
    "    \"\"\"\n",
    "    Test different time windows to see parameter stability.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for window in window_sizes:\n",
    "        if window <= len(observations):\n",
    "            # Try all possible windows of this size\n",
    "            for start in range(len(observations) - window + 1):\n",
    "                subset = observations[start:start + window]\n",
    "                params = estimate_growth_parameters(subset, reliability_level=reliability_level)\n",
    "                \n",
    "                start_date = subset[0][1]\n",
    "                end_date = subset[-1][1]\n",
    "                time_span = (end_date - start_date).days\n",
    "                \n",
    "                results.append({\n",
    "                    'window': window,\n",
    "                    'start_model': subset[0][0],\n",
    "                    'end_model': subset[-1][0],\n",
    "                    'time_span_days': time_span,\n",
    "                    'doubling_time': params[0],\n",
    "                    'acceleration': params[1]\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "print('Loaded libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be06616b-8538-41bc-96ae-c167a1b543c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 50% Reliability ===\n",
      "GPT‑2 to Claude 4 Opus (50%): (324, 0.9)\n",
      "GPT-3 to Claude 4 Opus (50%): (294, 0.9)\n",
      "GPT-4 to Claude 4 Opus (50%): (255, 0.9)\n",
      "Claude 3 Opus to Claude 4 Opus (50%): (109, 1.0)\n",
      "GPT-4 to o3 (50%): (256, 0.9)\n",
      "GPT‑4o to o3 (50%): (97, 1.0)\n",
      "o1 preview to o3 (50%): (107, 0.9)\n",
      "GPT-4 to Claude 4 Opus (50%): (255, 0.9)\n",
      "GPT‑4o to Claude 4 Opus (50%): (107, 1.0)\n",
      "o1 preview to Claude 4 Opus (50%): (126, 0.9)\n",
      "\n",
      "=== 80% Reliability ===\n",
      "GPT‑2 to Claude 4 Opus (80%): (213, 0.965)\n",
      "GPT-3 to Claude 4 Opus (80%): (301, 0.9)\n",
      "GPT-4 to Claude 4 Opus (80%): (235, 0.9)\n",
      "Claude 3 Opus to Claude 4 Opus (80%): (99, 1.0)\n",
      "GPT-4 to o3 (80%): (241, 0.9)\n",
      "GPT‑4o to o3 (80%): (115, 0.9)\n",
      "o1 preview to o3 (80%): (98, 0.9)\n",
      "GPT-4 to Claude 4 Opus (80%): (235, 0.9)\n",
      "GPT‑4o to Claude 4 Opus (80%): (122, 0.905)\n",
      "o1 preview to Claude 4 Opus (80%): (111, 0.9)\n"
     ]
    }
   ],
   "source": [
    "observed_models: List[Tuple[str, datetime, float, float]] = [\n",
    "    # model                       # release date         # task length at 50%  # task length at 80% (in hrs)\n",
    "    (\"GPT‑2\",                     datetime(2019, 2, 14),  2/3600,              0.1/3600),\n",
    "    (\"GPT-3\",                     datetime(2020, 5, 28),  9/3600,                2/3600),\n",
    "    (\"GPT‑3.5 Turbo\",             datetime(2023, 6, 13), 36/3600,               10/3600),\n",
    "    (\"GPT-4\",                     datetime(2023, 3, 14),  6/60,                  1/60  ),\n",
    "    (\"GPT-4-Nov23\",               datetime(2023, 11, 6),  8/60,                  1/60  ),\n",
    "    (\"Claude 3 Opus\",             datetime(2024, 3,  4),  6/60,                  1/60  ),\n",
    "    (\"GPT‑4o\",                    datetime(2024, 5, 13),  9/60,                  2/60  ),\n",
    "    (\"Claude 3.5 Sonnet (old)\",   datetime(2024, 6, 20), 18/60,                  3/60  ),\n",
    "    (\"o1 preview\",                datetime(2024, 9, 12), 22/60,                  4/60  ),\n",
    "    (\"Claude 3.5 Sonnet (new)\",   datetime(2024,10, 22), 28/60,                  5/60  ),\n",
    "    (\"o1\",                        datetime(2024,12,  5), 39/60,                  6/60  ),\n",
    "    (\"Claude 3.7 Sonnet\",         datetime(2025, 2, 24), 59/60,                 15/60  ),\n",
    "    (\"o3\",                        datetime(2025, 4, 16),  1+45/60,              20/60  ),\n",
    "    (\"Claude 4 Sonnet\",           datetime(2025, 5, 22),  1+7/60,               16/60  ),\n",
    "    (\"Claude 4 Opus\",             datetime(2025, 5, 22),  1+19/60,              20/60  ),\n",
    "]\n",
    "\n",
    "print(\"=== 50% Reliability ===\")\n",
    "print_estimation(observed_models)\n",
    "print_estimation(observed_models[1:])\n",
    "print_estimation(observed_models[3:])\n",
    "print_estimation(observed_models[5:])\n",
    "print_estimation(observed_models[3:-2])\n",
    "print_estimation(observed_models[6:-2])\n",
    "print_estimation(observed_models[8:-2])\n",
    "print_estimation(observed_models[3:])\n",
    "print_estimation(observed_models[6:])\n",
    "print_estimation(observed_models[8:])\n",
    "\n",
    "print(\"\\n=== 80% Reliability ===\")\n",
    "print_estimation(observed_models, \"80%\")\n",
    "print_estimation(observed_models[1:], \"80%\")\n",
    "print_estimation(observed_models[3:], \"80%\")\n",
    "print_estimation(observed_models[5:], \"80%\")\n",
    "print_estimation(observed_models[3:-2], \"80%\")\n",
    "print_estimation(observed_models[6:-2], \"80%\")\n",
    "print_estimation(observed_models[8:-2], \"80%\")\n",
    "print_estimation(observed_models[3:], \"80%\")\n",
    "print_estimation(observed_models[6:], \"80%\")\n",
    "print_estimation(observed_models[8:], \"80%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cf7ffa3-b6c1-4b28-98a3-f3e94b123585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bootstrap Analysis ===\n",
      "\n",
      "50% Reliability:\n",
      "Full dataset: (293, 0.9) (95% CI: {'doubling_time': (85, 332), 'acceleration': (0.9, 1.0)})\n",
      "2024+ models: (112, 1.0) (95% CI: {'doubling_time': (101, 165), 'acceleration': (0.9, 1.0)})\n",
      "\n",
      "80% Reliability:\n",
      "Full dataset: (231, 0.934) (95% CI: {'doubling_time': (95, 304), 'acceleration': (0.9, 1.0)})\n",
      "2024+ models: (105, 0.983) (95% CI: {'doubling_time': (95, 139), 'acceleration': (0.9, 1.0)})\n",
      "\n",
      "=== Parameter stability by time window ===\n",
      "       doubling_time             acceleration          \n",
      "                mean         std         mean       std\n",
      "window                                                 \n",
      "6            178.700  102.563855     0.940000  0.051640\n",
      "8            189.875   99.900576     0.947875  0.051482\n",
      "10           207.500   99.885434     0.933333  0.051640\n",
      "12           238.500  106.591119     0.925000  0.050000\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Bootstrap Analysis ===\")\n",
    "for rel in [\"50%\", \"80%\"]:\n",
    "    print(f\"\\n{rel} Reliability:\")\n",
    "    full_results = bootstrap_growth_parameters(observed_models, reliability_level=rel)\n",
    "    print(f\"Full dataset: {full_results['median']} (95% CI: {full_results['ci_95']})\")\n",
    "    recent_models = [m for m in observed_models if m[1] >= datetime(2024, 1, 1)]\n",
    "    recent_results = bootstrap_growth_parameters(recent_models, reliability_level=rel)\n",
    "    print(f\"2024+ models: {recent_results['median']} (95% CI: {recent_results['ci_95']})\")\n",
    "\n",
    "df = sliding_window_analysis(observed_models)\n",
    "print(\"\\n=== Parameter stability by time window ===\")\n",
    "print(df.groupby('window')[['doubling_time', 'acceleration']].agg(['mean', 'std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28ddf801-4942-4576-86d2-9e528062af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step |    Date    |    Day |       Task | τ (d)\n",
      "-----------------------------------------------\n",
      "   0 | 2019‑02‑14 |      0 |       2sec | 260.0\n",
      "   1 | 2019‑11‑01 |    260 |       4sec | 247.0\n",
      "   2 | 2020‑07‑05 |    507 |       8sec | 234.6\n",
      "   3 | 2021‑02‑24 |    741 |      16sec | 222.9\n",
      "   4 | 2021‑10‑05 |    964 |      32sec | 211.8\n",
      "   5 | 2022‑05‑05 |   1176 |    1.07min | 201.2\n",
      "   6 | 2022‑11‑22 |   1377 |    2.13min | 191.1\n",
      "   7 | 2023‑06‑01 |   1568 |    4.27min | 181.6\n",
      "   8 | 2023‑11‑30 |   1750 |    8.53min | 172.5\n",
      "   9 | 2024‑05‑20 |   1922 |   17.07min | 163.9\n",
      "  10 | 2024‑10‑31 |   2086 |   34.13min | 155.7\n",
      "  11 | 2025‑04‑05 |   2242 |     1.14hr | 147.9\n",
      "  12 | 2025‑08‑31 |   2390 |     2.28hr | 140.5\n",
      "  13 | 2026‑01‑18 |   2530 |     4.55hr | 133.5\n",
      "  14 | 2026‑06‑01 |   2664 |     9.10hr | 126.8\n",
      "  15 | 2026‑10‑05 |   2790 |    18.20hr | 120.5\n",
      "  16 | 2027‑02‑03 |   2911 |    36.41hr | 114.4\n",
      "  17 | 2027‑05‑28 |   3025 |    72.82hr | 108.7\n",
      "  18 | 2027‑09‑14 |   3134 |   145.64hr | 103.3\n",
      "  19 | 2027‑12‑26 |   3237 |   291.27hr |  98.1  <-- reached target\n"
     ]
    }
   ],
   "source": [
    "test_acceleration(\n",
    "    start_task_length=2/60/60, # GPT2\n",
    "    agi_task_length=167, \n",
    "    initial_doubling_time=260,\n",
    "    acceleration=0.95,\n",
    "    start_date=\"2019-02-14\", # GPT2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7fc23e-67dc-49f2-8163-1739c1b405f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
